{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a2175efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dgl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c74df097",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./train_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "with open('./test_data.pkl', 'rb') as f1:\n",
    "    data1 = pickle.load(f1)\n",
    "traindata = []\n",
    "testdata = []\n",
    "for i in data:\n",
    "    tmp = i.ndata['y'][0]\n",
    "    traindata.append((i, tmp))\n",
    "for j in data1:\n",
    "    tmp = j.ndata['y'][0]\n",
    "    testdata.append((j, tmp))\n",
    "\n",
    "def collate(samples):\n",
    "    # 输入`samples` 是一个列表\n",
    "    # 每个元素都是一个二元组 (图, 标签)\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, torch.Tensor(labels).type(torch.long)\n",
    "\n",
    "data_loader = DataLoader(traindata, batch_size=5, shuffle=True,\n",
    "                         collate_fn=collate)\n",
    "test_loader = DataLoader(testdata, batch_size=5, shuffle=True,\n",
    "                         collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c3eadce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 将节点表示h作为信息发出\n",
    "msg = fn.copy_src(src='feat', out='m')\n",
    "\n",
    "def reduce(nodes):\n",
    "    \"\"\"对所有邻节点节点特征求平均并覆盖原本的节点特征。\"\"\"\n",
    "    accum = torch.mean(nodes.mailbox['m'], 1)\n",
    "    return {'feat': accum}\n",
    "\n",
    "class NodeApplyModule(nn.Module):\n",
    "    \"\"\"将节点特征 hv 更新为 ReLU(Whv+b).\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['feat'])\n",
    "        h = self.activation(h)\n",
    "        return {'feat' : h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "2dddf800",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        # 使用 h 初始化节点特征。\n",
    "        g.ndata['feat'] = feature\n",
    "        # 使用 update_all接口和自定义的消息传递及累和函数更新节点表示。\n",
    "        g.update_all(msg, reduce)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('feat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ad80fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Regress(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(Regress, self).__init__()\n",
    "        # 两层图卷积层。\n",
    "        self.layers = nn.ModuleList([\n",
    "            GCNLayer(in_dim, hidden_dim, F.relu),\n",
    "            GCNLayer(hidden_dim, hidden_dim, F.relu),\n",
    "            GCNLayer(hidden_dim, hidden_dim, F.relu),\n",
    "            GCNLayer(hidden_dim, hidden_dim, F.relu)])\n",
    "        # 回归预测层。\n",
    "        self.emb = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.emb1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.regress = nn.Linear(hidden_dim, 5)\n",
    "\n",
    "    def forward(self, g):\n",
    "        # 初始数据\n",
    "        h = g.ndata['feat']\n",
    "        # 图卷积层。\n",
    "        for conv in self.layers:\n",
    "            h = conv(g, h)\n",
    "        g.ndata['feat'] = h\n",
    "        # 读出函数。\n",
    "        graph_repr = dgl.mean_nodes(g, 'feat')\n",
    "        # 回归预测层。\n",
    "        graph_repr = self.emb(graph_repr)\n",
    "        graph_repr = self.emb1(graph_repr)\n",
    "        return F.softmax(self.regress(graph_repr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "edab247e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-225-10dd0d367261>:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.regress(graph_repr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 1.6115\n",
      "Epoch 1, loss 1.6107\n",
      "Epoch 2, loss 1.6098\n",
      "Epoch 3, loss 1.6100\n",
      "Epoch 4, loss 1.6103\n",
      "Epoch 5, loss 1.6099\n",
      "Epoch 6, loss 1.6100\n",
      "Epoch 7, loss 1.6093\n",
      "Epoch 8, loss 1.6191\n",
      "Epoch 9, loss 1.6475\n",
      "Epoch 10, loss 1.6127\n",
      "Epoch 11, loss 1.6096\n",
      "Epoch 12, loss 1.6085\n",
      "Epoch 13, loss 1.6034\n",
      "Epoch 14, loss 1.5876\n",
      "Epoch 15, loss 1.5551\n",
      "Epoch 16, loss 1.5818\n",
      "Epoch 17, loss 1.5554\n",
      "Epoch 18, loss 1.5380\n",
      "Epoch 19, loss 1.5395\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "# 创建模型\n",
    "model = Regress(13, 128)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "\n",
    "epoch_losses = []\n",
    "tmp_loss = 1.61\n",
    "for epoch in range(20):\n",
    "    epoch_loss = 0\n",
    "    for iter, (bg, label) in enumerate(data_loader):\n",
    "        prediction = model(bg)\n",
    "        loss = loss_func(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "    epoch_loss /= (iter + 1)\n",
    "    if epoch_loss <= tmp_loss:\n",
    "        tmp_loss = epoch_loss\n",
    "        model_res = model\n",
    "    print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n",
    "    epoch_losses.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "00bda9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.4885\n",
      "tensor(12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-225-10dd0d367261>:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.regress(graph_repr))\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model_res.eval()\n",
    "    epoch_loss = 0\n",
    "    erer = 0\n",
    "    for iter, (bg, label) in enumerate(test_loader):\n",
    "        prediction = model_res(bg)\n",
    "        oooo = torch.sum(prediction.argmax(dim=1) == label)\n",
    "        erer += oooo\n",
    "        loss = loss_func(prediction, label)\n",
    "        epoch_loss += loss.detach().item()\n",
    "    epoch_loss /= (iter + 1)\n",
    "    print('loss {:.4f}'.format(epoch_loss))\n",
    "    print(erer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
